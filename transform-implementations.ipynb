{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":221330082,"sourceType":"kernelVersion"}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport cupy as cp\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nimport timeseriesanalysis as tsa\n!pip install bezier\n!pip install lmoments3\nimport lmoments3 as lm\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom numba import njit\nimport itertools \nfrom scipy.special import comb\nfrom scipy import signal","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-04T16:24:28.859723Z","iopub.execute_input":"2025-03-04T16:24:28.860022Z","iopub.status.idle":"2025-03-04T16:24:58.440239Z","shell.execute_reply.started":"2025-03-04T16:24:28.859999Z","shell.execute_reply":"2025-03-04T16:24:58.439305Z"}},"outputs":[{"name":"stdout","text":"Collecting bezier\n  Downloading bezier-2024.6.20-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\nCollecting numpy>=2.0.0 (from bezier)\n  Downloading numpy-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bezier-2024.6.20-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading numpy-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy, bezier\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncatboost 1.2.7 requires numpy<2.0,>=1.16.0, but you have numpy 2.2.3 which is incompatible.\ncudf-cu12 24.12.0 requires pyarrow<19.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 19.0.0 which is incompatible.\ncupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.2.3 which is incompatible.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.3 which is incompatible.\nlangchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\", but you have async-timeout 5.0.1 which is incompatible.\nlangchain 0.3.12 requires numpy<2,>=1.22.4; python_version < \"3.12\", but you have numpy 2.2.3 which is incompatible.\nmatplotlib 3.7.5 requires numpy<2,>=1.20, but you have numpy 2.2.3 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.3 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.3 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.3 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.3 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\npytensor 2.26.4 requires numpy<2,>=1.17.0, but you have numpy 2.2.3 which is incompatible.\ntensorflow 2.17.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.2.3 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\nthinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.2.3 which is incompatible.\nydata-profiling 4.12.2 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bezier-2024.6.20 numpy-2.2.3\nCollecting lmoments3\n  Downloading lmoments3-1.0.8-py3-none-any.whl.metadata (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lmoments3) (2.2.3)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lmoments3) (1.13.1)\nDownloading lmoments3-1.0.8-py3-none-any.whl (48 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: lmoments3\nSuccessfully installed lmoments3-1.0.8\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# **Kernel Function Computations For GPUs**","metadata":{}},{"cell_type":"code","source":"def remove_duplicate_arrays(array_list):\n    \"\"\"\n    Removes duplicate NumPy arrays from a list, returning a new list\n    containing only unique arrays.\n\n    Args:\n        array_list (list of numpy arrays): A list containing NumPy arrays.\n\n    Returns:\n        list of numpy arrays: A new list containing only the unique NumPy arrays\n                              from the input list. The order of arrays in the\n                              returned list is preserved from the original list\n                              as much as possible (the *first* occurrence of each\n                              unique array is kept).\n    \"\"\"\n\n    unique_arrays = []\n    seen = set()  # Use a set to track seen array's byte representations\n\n    for arr in array_list:\n        # Convert the array to bytes for efficient set comparison\n        arr_bytes = arr.tobytes()\n\n        if arr_bytes not in seen:\n            unique_arrays.append(arr)\n            seen.add(arr_bytes)\n\n    return unique_arrays\n    \n\ndef bezier_curve_fast_cupy(control_points, num_points=100):\n    \"\"\"\n    Generates 2D Bézier curves for a batch of control point arrangements,\n    optimized for speed using CuPy, and scales each curve so that its points sum to 1.\n\n    Args:\n        control_points (cupy array): A CuPy array of shape (num_arrangements, n+1, 2)\n            representing the control points for each Bézier curve.\n        num_points (int): The number of points to generate along each curve.\n\n    Returns:\n        cupy array: A CuPy array of shape (num_arrangements, num_points, 2) containing the (x, y)\n            coordinates of the points on the Bézier curves, scaled to sum to 1.\n    \"\"\"\n\n    n = control_points.shape[1] - 1\n    num_arrangements = control_points.shape[0]\n\n    # Precompute Bernstein basis polynomials for all t values\n    t = cp.linspace(0, 1, num_points)[:, None]\n\n    # Precompute the binomial coefficients (on the CPU, then transfer)\n    binomial_coefficients = np.array([comb(n, i) for i in range(n + 1)])\n    binomial_coefficients_gpu = cp.asarray(binomial_coefficients)\n\n    # Vectorized calculation of Bernstein polynomials\n    bernstein_polynomials = binomial_coefficients_gpu * (t ** cp.arange(n + 1).reshape(1, -1)) * ((1 - t) ** cp.arange(n, -1, -1).reshape(1, -1))\n\n    # Calculate curve points using vectorized operations (Batch Matrix Multiplication)\n    points = cp.matmul(bernstein_polynomials, control_points)\n\n    # Scale the curve to sum to 1\n    curve_sum = cp.sum(points, axis=1, keepdims=True)\n    mask = (curve_sum != 0)\n    points = cp.where(mask, points / curve_sum, points)\n\n    return points.astype(cp.float16)\n\n\n\ndef generate_coordinate_arrangements_cupy(point_ranges):\n    \"\"\"\n    Generates all possible coordinate arrangements for a specified number of points,\n    where each point can have its own independent x and y ranges.\n\n    Args:\n        point_ranges (list of tuples): A list of tuples, where each tuple represents the\n            (x_range, y_range) for a single point.  Each x_range and y_range are themselves\n            tuples specifying (min_x, max_x) and (min_y, max_y), respectively.\n\n            For example:\n            `point_ranges = [((0, 2), (0, 3)), ((1, 4), (2, 5)), ((0, 1), (1, 1))]`\n            means:\n                - Point 1: x can be 0, 1, or 2; y can be 0, 1, 2, or 3.\n                - Point 2: x can be 1, 2, 3, or 4; y can be 2, 3, 4, or 5.\n                - Point 3: x can be 0 or 1; y can only be 1.\n\n    Returns:\n        A CuPy array of shape (num_arrangements, num_points, 2).\n    \"\"\"\n    arrangements = []\n    num_points = len(point_ranges)\n\n    def generate_recursive(current_arrangement, point_index):\n        if point_index == num_points:\n            arrangements.append(np.array(current_arrangement))\n            return\n\n        x_range, y_range = point_ranges[point_index]\n        for x in range(x_range[0], x_range[1] + 1):\n            for y in range(y_range[0], y_range[1] + 1):\n                new_arrangement = current_arrangement + [[x, y]]\n                generate_recursive(new_arrangement, point_index + 1)\n\n    generate_recursive([], 0)\n    return cp.asarray(arrangements)\n\n\n# Example Usage:\n# Define the individual x and y ranges for each point\n\nmin_x = 0\nmax_x = 4\nmin_y = 0\nmax_y = 7\n\npoint_ranges = [\n    ((min_x, min_x), (min_y, max_y)),\n    ((min_x, max_x), (min_y, max_y)),\n    ((min_x, max_x), (min_y, max_y)),\n    ((max_x, max_x), (min_y, max_y))  \n]\n\n# Generate all arrangements\nall_arrangements_gpu = generate_coordinate_arrangements_cupy(point_ranges)\n\nmin_window_size = 10\nmax_window_size = 1000\nwindow_size_step = 15\nloop_points = range(min_window_size, max_window_size, window_size_step)\nenum_loop_points = enumerate(loop_points)\n\n# Preallocate the array to store the kernel functions (on the CPU)\nkernel_functions = [] # Changed from np.empty to a list\n\nwith tqdm(desc=\"Processing\") as pbar:\n    for window_index, window_size in enum_loop_points:\n        # Calculate all Bezier curves for the current window size in one go!\n        all_curves = bezier_curve_fast_cupy(all_arrangements_gpu, num_points=window_size) #shape is num_arrangements, num_points, 2\n        y_coords_gpu = all_curves[:, :, 1]  # Extract y-coordinates; shape is num_arrangements, num_points\n\n        # Copy y-coordinates to CPU as a NumPy array\n        y_coords_cpu = cp.asnumpy(y_coords_gpu)  #Shape is now num_arrangements, num_points\n\n        #Store NumPy Array as a 2D array in the list.\n        kernel_functions.append(y_coords_cpu) #Changed to append, not assign\n\n        pbar.update(1)\n\n# leave unique kernel functions only\nfor i in range(len(kernel_functions)):\n    kernel_functions[i] = remove_duplicate_arrays(kernel_functions[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T22:50:34.315491Z","iopub.execute_input":"2025-03-03T22:50:34.315868Z","iopub.status.idle":"2025-03-03T22:50:52.630313Z","shell.execute_reply.started":"2025-03-03T22:50:34.315834Z","shell.execute_reply":"2025-03-03T22:50:52.629572Z"}},"outputs":[{"name":"stderr","text":"Processing: 66it [00:11,  5.77it/s]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **Kernel Function Computations For TPUs**","metadata":{}},{"cell_type":"code","source":"tfd = tfp.distributions\n\ndef bezier_curve_fast_tf(control_points, num_points=100):\n    \"\"\"\n    Generates 2D Bézier curves for a batch of control point arrangements,\n    optimized for speed using TensorFlow and TPUs, and scales each curve so that its points sum to 1.\n\n    Args:\n        control_points (tf.Tensor): A TensorFlow tensor of shape (num_arrangements, n+1, 2)\n            representing the control points for each Bézier curve.\n        num_points (int): The number of points to generate along each curve.\n\n    Returns:\n        tf.Tensor: A TensorFlow tensor of shape (num_arrangements, num_points, 2) containing the (x, y)\n            coordinates of the points on the Bézier curves, scaled to sum to 1.\n    \"\"\"\n\n    n = tf.cast(tf.shape(control_points)[1] - 1, dtype=tf.float32)  # Use tf.cast\n    num_arrangements = tf.shape(control_points)[0]\n\n    # Precompute Bernstein basis polynomials for all t values\n    t = tf.linspace(0.0, 1.0, num_points)[:, None]\n\n    # Precompute the binomial coefficients (on the CPU, then transfer if needed - usually XLA handles this fine)\n    binomial_coefficients = np.array([comb(int(n), i) for i in range(int(n) + 1)])\n    binomial_coefficients_tf = tf.constant(binomial_coefficients, dtype=tf.float32) # Convert to tf.constant\n\n    # Vectorized calculation of Bernstein polynomials\n    arange_n_plus_1 = tf.range(int(n + 1), dtype=tf.float32)\n    bernstein_polynomials = binomial_coefficients_tf * (t ** arange_n_plus_1) * ((1 - t) ** tf.reverse(arange_n_plus_1, axis=[0]))\n\n    # Calculate curve points using vectorized operations (Batch Matrix Multiplication)\n    bernstein_polynomials = tf.cast(bernstein_polynomials, dtype=tf.float32)\n    control_points = tf.cast(control_points, dtype=tf.float32)\n    points = tf.matmul(bernstein_polynomials, control_points)\n\n    # Scale the curve to sum to 1\n    curve_sum = tf.reduce_sum(points, axis=1, keepdims=True)\n    mask = tf.not_equal(curve_sum, 0)\n    points = tf.where(mask, points / curve_sum, points)\n\n    return tf.cast(points, dtype=tf.float16)\n\n\ndef generate_coordinate_arrangements_tf():\n    \"\"\"\n    Generates all possible coordinate arrangements for four points, as a TensorFlow tensor.\n\n    Returns:\n        A TensorFlow tensor of shape (num_arrangements, 4, 2).\n    \"\"\"\n    arrangements = []\n    for y1 in range(0, 7):\n        for x2 in range(0, 5):\n            for y2 in range(0, 7):\n                for x3 in range(0, 5):\n                    for y3 in range(0, 7):\n                        for y4 in range(0, 7):\n                            point1 = [0, y1]\n                            point2 = [x2, y2]\n                            point3 = [x3, y3]\n                            point4 = [4, y4]\n                            arrangements.append(np.array([point1, point2, point3, point4]))\n\n    return tf.constant(np.array(arrangements), dtype=tf.float32)\n\n\n# TPU Strategy Setup (REQUIRED if running on TPU)\n# This part needs to be adjusted based on your specific TPU setup\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default strategy in case of no TPU\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\n# Now, wrap everything that uses the TPU inside the strategy scope\nwith strategy.scope():\n    # Generate all arrangements\n    all_arrangements_tf = generate_coordinate_arrangements_tf()\n\n    min_window_size = 10\n    max_window_size = 1000\n    window_size_step = 15\n    loop_points = range(min_window_size, max_window_size, window_size_step)\n    enum_loop_points = enumerate(loop_points)\n\n    # Preallocate the array to store the kernel functions (on the CPU)\n    kernel_functions = np.empty((len(loop_points), len(all_arrangements_tf)), dtype=object)\n\n\n    with tqdm(desc=\"Processing\") as pbar:\n        for window_index, window_size in enum_loop_points:\n            # Calculate all Bezier curves for the current window size in one go!\n            all_curves = bezier_curve_fast_tf(all_arrangements_tf, num_points=window_size) #shape is num_arrangements, num_points, 2\n            y_coords_tf = all_curves[:, :, 1]  # Extract y-coordinates; shape is num_arrangements, num_points\n\n            # Convert y_coords_tf to NumPy array\n            y_coords_np = y_coords_tf.numpy()\n\n            # Store NumPy Array in 2d output.\n            kernel_functions[window_index, :] = [y_coords_np[i, :] for i in range(y_coords_np.shape[0])]\n\n            pbar.update(1)\n\nprint(\"Finished processing.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Computations for Multiscale LTI Filter Information","metadata":{}},{"cell_type":"code","source":"def conv_subtract_scale(time_series, kernel_functions):\n    \"\"\"\n    Performs convolution, subtraction, and scaling operations efficiently, \n    minimizing data conversions.\n\n    Args:\n        time_series: A 1D NumPy array of time series data.\n        kernel_functions: A 2D NumPy array of kernel functions.\n\n    Returns:\n        A TensorFlow tensor representing the result of the operations.\n    \"\"\"\n\n    # 1. Preprocessing & Reshaping in NumPy (Optimize for TF Input)\n    time_series = np.expand_dims(time_series, axis=(0, -1))  # (1, time_series_length, 1)\n    kernel_functions = np.expand_dims(kernel_functions, axis=1)  # (num_kernel_functions, 1, kernel_length)\n    kernel_functions = np.transpose(kernel_functions, (2, 1, 0))  # (kernel_length, 1, num_kernel_functions)\n\n    # 2. Convert to TensorFlow tensors\n    time_series_tf = tf.convert_to_tensor(time_series, dtype=tf.float16)\n    kernel_functions_tf = tf.convert_to_tensor(kernel_functions, dtype=tf.float16)\n\n    # 3. Convolution in TensorFlow\n    convolved_tf = tf.nn.convolution(\n        input=time_series_tf,\n        filters=kernel_functions_tf,\n        padding='VALID',\n        strides=1,\n    )\n    convolved_tf = tf.squeeze(convolved_tf, axis=0)  # Remove batch dim\n\n    # 4. Reshape time_series_tf for subtraction (all in TensorFlow)\n    time_series_tf = tf.reshape(time_series_tf, [time_series_tf.shape[1]])  #Now remove the dims for the tf tensor\n    starting_index = int(time_series_tf.shape[0] - convolved_tf.shape[0])\n    time_series_sliced_tf = time_series_tf[starting_index:] #Slice now in TF\n\n    # 5. Reshape for subtraction (all in TensorFlow)\n    # Determine if convolved_tf needs to be reshaped based on time_series_sliced_tf's shape\n    if time_series_sliced_tf.shape[0] == convolved_tf.shape[1]:\n        time_series_reshaped_tf = tf.reshape(time_series_sliced_tf, [1, -1])\n    elif time_series_sliced_tf.shape[0] == convolved_tf.shape[0]:\n        time_series_reshaped_tf = tf.reshape(time_series_sliced_tf, [-1, 1])\n    else:\n        raise ValueError(\"Arrays are not compatible. time_series must match either the number of rows or columns of filter outputs.\")\n    \n    # 6. Perform subtraction *directly in TensorFlow* (No CuPy conversion anymore!)\n    intermediate_result = time_series_reshaped_tf - convolved_tf\n    scaled_result = intermediate_result.numpy()\n    with tqdm(desc=\"Processing\") as pbar:\n        for i in range(scaled_result.shape[1]):\n            scaled_result[:,i] /= np.std(scaled_result[:,i])\n            pbar.update(1)\n            \n    return scaled_result\n\n# Example Time Series\nlength = 5000\nsample_rate = 10\ntime_series = tsa.symmetric_random_walk(length, sample_rate)\nresid = conv_subtract_scale(time_series, kernel_functions[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T22:52:01.257479Z","iopub.execute_input":"2025-03-03T22:52:01.257836Z","iopub.status.idle":"2025-03-03T22:52:02.256288Z","shell.execute_reply.started":"2025-03-03T22:52:01.257808Z","shell.execute_reply":"2025-03-03T22:52:02.255535Z"}},"outputs":[{"name":"stderr","text":"Processing: 0it [00:00, ?it/s]/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:187: RuntimeWarning: overflow encountered in reduce\nProcessing: 3746it [00:00, 4036.02it/s]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Computing L-Moments","metadata":{}},{"cell_type":"code","source":"import lmoments3 as lm\n\ndef sliding_lmoments(data, w, nmom):\n    \"\"\"\n    Compute L-moments in a sliding window over the data using the lmoments3 package.\n\n    Parameters:\n    - data (list or array): Input data sequence (e.g., time series or list of numbers).\n    - w (int): Window size (must be >= nmom).\n    - nmom (int): Number of L-moments to compute (e.g., 1 for λ₁, 2 for λ₁ and λ₂, etc.).\n\n    Returns:\n    - list: List of lists, where each sublist contains the first nmom L-moments\n            [λ₁, λ₂, ..., λ_nmom] for each window.\n\n    Requires:\n    - lmoments3 package (install via `pip install lmoments3`)\n\n    Raises:\n    - ValueError: If window size w is less than nmom.\n    \"\"\"\n    if w < nmom:\n        raise ValueError(\"Window size w must be at least nmom\")\n    \n    n = len(data)\n    result = []\n    \n    # Slide the window over the data\n    for i in range(n - w + 1):\n        window_data = data[i:i + w]\n        # Compute L-moments and ratios using lmoments3\n        ratios = lm.lmom_ratios(window_data, nmom)\n        # Extract actual L-moments: λ₁ and λ₂ are direct, λ₃ onward are computed from ratios\n        l_moments = [ratios[0], ratios[1]] + [ratios[k] * ratios[1] for k in range(2, nmom)]\n        result.append(l_moments)\n    \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T16:32:33.110275Z","iopub.execute_input":"2025-03-04T16:32:33.110576Z","iopub.status.idle":"2025-03-04T16:32:33.115885Z","shell.execute_reply.started":"2025-03-04T16:32:33.110554Z","shell.execute_reply":"2025-03-04T16:32:33.115033Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Install lmoments3 first: pip install lmoments3\ndata = [1, 2, 3, 4, 5]\nw = 3\nnmom = 2\n\nresult = sliding_lmoments(data, w, nmom)\nfor i, lmom in enumerate(result):\n    print(f\"Window {i}: λ₁ = {lmom[0]:.3f}, λ₂ = {lmom[1]:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T16:32:24.571837Z","iopub.execute_input":"2025-03-04T16:32:24.572289Z","iopub.status.idle":"2025-03-04T16:32:24.579659Z","shell.execute_reply.started":"2025-03-04T16:32:24.572257Z","shell.execute_reply":"2025-03-04T16:32:24.578556Z"}},"outputs":[{"name":"stdout","text":"Window 0: λ₁ = 2.000, λ₂ = 0.667\nWindow 1: λ₁ = 3.000, λ₂ = 0.667\nWindow 2: λ₁ = 4.000, λ₂ = 0.667\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def find_equal_arrays(array_list):\n    \"\"\"\n    Finds and groups NumPy arrays in a list that are equal to each other.\n\n    Args:\n        array_list (list of numpy arrays): A list containing NumPy arrays.\n\n    Returns:\n        list of lists: A list where each inner list contains the *indices* of arrays\n                       in the input `array_list` that are equal to each other.  \n                       Arrays are considered equal if all their elements are equal.\n\n                       For example, if array_list = [array1, array2, array3]\n                       and array1 and array3 are equal, the function might return:\n                       [[0, 2], [1]]  (array1 and array3 are at indices 0 and 2, array2 at 1)\n\n                       or\n                       [[1], [0, 2]]\n\n                       The order of the inner lists and the order of indices within the inner\n                       lists is not guaranteed.\n    \"\"\"\n\n    equal_groups = []\n    remaining_indices = list(range(len(array_list)))  # Indices to process\n\n    while remaining_indices:\n        first_index = remaining_indices.pop(0)  # Take the first unprocessed array\n        equal_group = [first_index]\n        first_array = array_list[first_index]\n\n        # Compare to all remaining unprocessed arrays\n        indices_to_remove = []\n        for i in remaining_indices:\n            other_array = array_list[i]\n            if np.array_equal(first_array, other_array):\n                equal_group.append(i)\n                indices_to_remove.append(i)\n\n        # Remove the matched indices from remaining_indices (in reverse order to avoid shifting issues)\n        for i in sorted(indices_to_remove, reverse=True):\n            remaining_indices.remove(i)\n\n        equal_groups.append(equal_group)\n\n    return equal_groups\n\n\n# Example Usage:\narray1 = np.array([1, 2, 3])\narray2 = np.array([4, 5, 6])\narray3 = np.array([1, 2, 3])  # Equal to array1\narray4 = np.array([7, 8, 9])\narray5 = np.array([4, 5, 6])  # Equal to array2\narray6 = np.array([1, 2, 3])  # Equal to array1 and array3\n\nmy_list = [array1, array2, array3, array4, array5, array6]\nequality_groups = find_equal_arrays(my_list)\nprint(f\"Arrays equal to each other (grouped by indices): {equality_groups}\")\n\n\narray7 = np.array([1, 1, 1])\narray8 = np.array([1, 1, 1])\narray9 = np.array([2, 2, 2])\n\nmy_list2 = [array7, array8, array9]\nequality_groups2 = find_equal_arrays(my_list2)\nprint(f\"Arrays equal to each other (grouped by indices) 2: {equality_groups2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:54:38.541243Z","iopub.execute_input":"2025-03-03T15:54:38.541556Z","iopub.status.idle":"2025-03-03T15:54:38.549762Z","shell.execute_reply.started":"2025-03-03T15:54:38.541509Z","shell.execute_reply":"2025-03-03T15:54:38.548931Z"}},"outputs":[{"name":"stdout","text":"Arrays equal to each other (grouped by indices): [[0, 2, 5], [1, 4], [3]]\nArrays equal to each other (grouped by indices) 2: [[0, 1], [2]]\n","output_type":"stream"}],"execution_count":2}]}